{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01b9c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94e617ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, TextStreamer, BitsAndBytesConfig, TextIteratorStreamer\n",
    "import torch\n",
    "import gradio as gr\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d107d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6040b9cdb5b1410d8337e1a4e28eba6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "824807b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA = \"meta-llama/Llama-3.2-3B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca38d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # Load model in 4-bit\n",
    "    bnb_4bit_quant_type=\"nf4\", # Use NF4 quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Compute in bfloat16\n",
    "    bnb_4bit_use_double_quant=True, # Enable nested quantization\n",
    "  )\n",
    "\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', quantization_config= quant_config)\n",
    "  return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fb6b4ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b799cbc2fc004dc2b77ed66aaa86fdb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer, model = load_model(LLAMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdc6c33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"}\n",
    "]\n",
    "\n",
    "user_input = \"What is Generative AI and Agentic AI?\"\n",
    "\n",
    "max_tokens = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcdb6a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stream_model(tokenizer, model, user_input, max_tokens=2000):\n",
    "\n",
    "  global messages\n",
    "\n",
    "  messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "  inputs = tokenizer.apply_chat_template(messages, return_tensors='pt').to('cuda')\n",
    "\n",
    "  streamer = TextIteratorStreamer(\n",
    "      tokenizer,\n",
    "      skip_prompt=True,\n",
    "      decode_kwargs={\"skip_special_token\":True}\n",
    "  )\n",
    "\n",
    "  thread = threading.Thread(\n",
    "      target = model.generate,\n",
    "      kwargs={'inputs':inputs, 'max_new_tokens':max_tokens, 'streamer':streamer}\n",
    "  )\n",
    "  thread.start()\n",
    "\n",
    "  for text_chunk in streamer:\n",
    "    filtered_chunk = text_chunk.replace(\"<|eot_id|>\",\"\")\n",
    "    print(filtered_chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95787c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Generative AI and Agentic AI are two related but distinct concepts in the field of Artificial Intelligence (AI).\n",
      "\n",
      "**Generative AI:**\n",
      "Generative AI refers to a type of AI that can generate new, original content, such as images, videos, music, text, or even entire systems. This is achieved through the use of algorithms that learn patterns and relationships in existing data, and then use this knowledge to create new, synthetic content that is similar in style and structure.\n",
      "\n",
      "Examples of Generative AI include:\n",
      "\n",
      "* Image generation: using neural networks to generate realistic images of objects, scenes, or people.\n",
      "* Music generation: using algorithms to create new music that sounds similar to existing music.\n",
      "* Text generation: using language models to generate new text, such as articles, stories, or even entire books.\n",
      "\n",
      "**Agentic AI:**\n",
      "Agentic AI, also known as \"agent-based\" AI, refers to a type of AI that can act autonomously and make decisions in a complex, dynamic environment. This type of AI is designed to be self-aware, autonomous, and proactive, rather than simply responding to input or stimuli.\n",
      "\n",
      "Agentic AI is often characterized by its ability to:\n",
      "\n",
      "* Learn and adapt to new situations\n",
      "* Make decisions based on goals and objectives\n",
      "* Interact with other agents or systems\n",
      "* Exhibit creativity and innovation\n",
      "\n",
      "Examples of Agentic AI include:\n",
      "\n",
      "* Autonomous vehicles: using AI to navigate and make decisions in real-time.\n",
      "* Robotics: using AI to control and interact with physical systems.\n",
      "* Financial trading: using AI to make predictions and decisions about market trends.\n",
      "\n",
      "**Relationship between Generative AI and Agentic AI:**\n",
      "While Generative AI focuses on creating new content, Agentic AI focuses on creating autonomous, self-aware systems that can interact with the world. In other words, Generative AI is about generating new content, while Agentic AI is about generating new capabilities and behaviors.\n",
      "\n",
      "However, the two concepts are not mutually exclusive, and there is a growing area of research that combines Generative AI and Agentic AI to create more advanced, autonomous systems that can generate new content and interact with the world in complex and dynamic ways.\n",
      "\n",
      "For example, an agentic AI system might use Generative AI to generate new content, such as images or music, that it can then use to interact with the world. Or, a Generative AI system might use Agentic AI to generate new capabilities and behaviors that allow it to interact with the world in more complex and dynamic ways.\n",
      "\n",
      "Overall, both Generative AI and Agentic AI are important areas of research in AI, and they have the potential to transform the way we interact with technology and each other."
     ]
    }
   ],
   "source": [
    "generate_stream_model(tokenizer, model, user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c397a47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gradio_opt(user_input):\n",
    "\n",
    "  global tokenizer, model, messages, max_tokens\n",
    "\n",
    "  messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "  inputs = tokenizer.apply_chat_template(messages, return_tensors='pt').to('cuda')\n",
    "\n",
    "  streamer = TextIteratorStreamer(\n",
    "      tokenizer,\n",
    "      skip_prompt=True,\n",
    "      decode_kwargs={\"skip_special_token\":True}\n",
    "  )\n",
    "\n",
    "  thread = threading.Thread(\n",
    "      target = model.generate,\n",
    "      kwargs={'inputs':inputs, 'max_new_tokens':max_tokens, 'streamer':streamer}\n",
    "  )\n",
    "  thread.start()\n",
    "\n",
    "  gather_reply = \"\"\n",
    "\n",
    "  for text_chunk in streamer:\n",
    "\n",
    "    filtered_chunk = text_chunk.replace(\"<|eot_id|>\",\"\")\n",
    "\n",
    "    gather_reply += filtered_chunk\n",
    "\n",
    "    yield gather_reply\n",
    "\n",
    "\n",
    "  messages.append({\"role\": \"assistant\", \"content\": gather_reply})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ed115ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "* Running on public URL: https://dd92736f1dd88d1b12.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://dd92736f1dd88d1b12.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://dd92736f1dd88d1b12.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "  gr.Markdown(\"# Chat with LLAMA\")\n",
    "  with gr.Row():\n",
    "    with gr.Column():\n",
    "      user_input = gr.Textbox(label=\"Your Message Here\", placeholder=\"Type Something here....\")\n",
    "      output_box = gr.Markdown(label=\"AI Response\", min_height=50)\n",
    "      send_button = gr.Button(\"Send\")\n",
    "\n",
    "  send_button.click(fn=generate_gradio_opt, inputs=user_input, outputs=output_box)\n",
    "\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
