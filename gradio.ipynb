{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b9c850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e617ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, TextStreamer, BitsAndBytesConfig, TextIteratorStreamer\n",
    "import torch\n",
    "import gradio as gr\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d107d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7e4b8242cc45f4a1794167cbd4bd5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "824807b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA = \"meta-llama/Llama-3.2-3B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca38d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # Load model in 4-bit\n",
    "    bnb_4bit_quant_type=\"nf4\", # Use NF4 quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Compute in bfloat16\n",
    "    bnb_4bit_use_double_quant=True, # Enable nested quantization\n",
    "  )\n",
    "\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', quantization_config= quant_config)\n",
    "  return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fb6b4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59e3a7d237e4feb95e1998fcd9336e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447c6e8ff84b45c3b8541617c3626e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a411f2915c54220840a85ff619ad878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cbf60adde8d4650a50352a2f752b368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11147980cfdc43bf9f55408fe7c0e154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2526990c1fca4e0d8a2a29b598c0d92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ef8e38d44f4c12b3762c52ca7148db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f3d613c4a14fad9e6635803cebc55e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1632a630f0f44e1697de82806dfb1ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc78cb96660c49949ab170417049e737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer, model = load_model(LLAMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdc6c33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"}\n",
    "]\n",
    "\n",
    "user_input = \"What is Generative AI and Agentic AI?\"\n",
    "\n",
    "max_tokens = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcdb6a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stream_model(tokenizer, model, user_input, max_tokens=2000):\n",
    "\n",
    "  global messages\n",
    "\n",
    "  messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "  inputs = tokenizer.apply_chat_template(messages, return_tensors='pt').to('cuda')\n",
    "\n",
    "  streamer = TextIteratorStreamer(\n",
    "      tokenizer,\n",
    "      skip_prompt=True,\n",
    "      decode_kwargs={\"skip_special_token\":True}\n",
    "  )\n",
    "\n",
    "  thread = threading.Thread(\n",
    "      target = model.generate,\n",
    "      kwargs={'inputs':inputs, 'max_new_tokens':max_tokens, 'streamer':streamer}\n",
    "  )\n",
    "  thread.start()\n",
    "\n",
    "  for text_chunk in streamer:\n",
    "    filtered_chunk = text_chunk.replace(\"<|eot_id|>\",\"\")\n",
    "    print(filtered_chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95787c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Generative AI and Agentic AI are two related but distinct concepts in the field of artificial intelligence (AI).\n",
      "\n",
      "**Generative AI:**\n",
      "\n",
      "Generative AI refers to a type of AI that can generate new, original content, such as images, videos, text, or music, based on patterns, styles, or data it has learned from existing content. This is often achieved through machine learning algorithms that can learn to replicate the characteristics of existing data.\n",
      "\n",
      "Examples of generative AI include:\n",
      "\n",
      "* Image generation: generating realistic images from scratch using techniques like Generative Adversarial Networks (GANs) or Variational Autoencoders (VAEs)\n",
      "* Text generation: generating coherent and contextually relevant text based on a given prompt or dataset\n",
      "* Music generation: generating new music compositions based on patterns and styles learned from existing music\n",
      "\n",
      "Generative AI has many applications, such as:\n",
      "\n",
      "* Art and design: generating new art pieces, designs, or patterns\n",
      "* Content creation: generating content for social media, advertising, or entertainment\n",
      "* Data augmentation: generating new data to augment existing datasets\n",
      "\n",
      "**Agentic AI:**\n",
      "\n",
      "Agentic AI, also known as Autonomous AI or Cognitive AI, refers to a type of AI that can make decisions, take actions, and interact with its environment in a more human-like way. This is often achieved through machine learning algorithms that can learn to predict and adapt to changing situations.\n",
      "\n",
      "Examples of agentic AI include:\n",
      "\n",
      "* Robotics: controlling robots to perform tasks that require decision-making and adaptation\n",
      "* Autonomous vehicles: navigating through complex environments and making decisions in real-time\n",
      "* Healthcare: developing AI systems that can diagnose and treat patients based on their symptoms and medical history\n",
      "\n",
      "Agentic AI has many applications, such as:\n",
      "\n",
      "* Decision-making: making decisions in complex, uncertain environments\n",
      "* Problem-solving: solving problems that require creativity and adaptability\n",
      "* Human-computer interaction: developing AI systems that can interact with humans in a more natural and intuitive way\n",
      "\n",
      "**Relationship between Generative AI and Agentic AI:**\n",
      "\n",
      "While generative AI focuses on creating new content, agentic AI focuses on making decisions and taking actions. However, these two concepts are not mutually exclusive. In fact, agentic AI can benefit from generative AI by generating new data, patterns, or styles to improve its decision-making and problem-solving capabilities.\n",
      "\n",
      "For example, an agentic AI system that uses generative AI to generate new images or music can use those generated content to inform its decision-making and take actions in a more informed and adaptive way.\n",
      "\n",
      "In summary, generative AI and agentic AI are two related but distinct concepts in the field of AI. Generative AI focuses on creating new content, while agentic AI focuses on making decisions and taking actions. However, the two concepts can complement each other, and the development of agentic AI can benefit from advances in generative AI."
     ]
    }
   ],
   "source": [
    "generate_stream_model(tokenizer, model, user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c397a47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gradio_opt(user_input):\n",
    "\n",
    "  global tokenizer, model, messages, max_tokens\n",
    "\n",
    "  messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "  inputs = tokenizer.apply_chat_template(messages, return_tensors='pt').to('cuda')\n",
    "\n",
    "  streamer = TextIteratorStreamer(\n",
    "      tokenizer,\n",
    "      skip_prompt=True,\n",
    "      decode_kwargs={\"skip_special_token\":True}\n",
    "  )\n",
    "\n",
    "  thread = threading.Thread(\n",
    "      target = model.generate,\n",
    "      kwargs={'inputs':inputs, 'max_new_tokens':max_tokens, 'streamer':streamer}\n",
    "  )\n",
    "  thread.start()\n",
    "\n",
    "  gather_reply = \"\"\n",
    "\n",
    "  for text_chunk in streamer:\n",
    "\n",
    "    filtered_chunk = text_chunk.replace(\"<|eot_id|>\",\"\")\n",
    "\n",
    "    gather_reply += filtered_chunk\n",
    "\n",
    "    yield gather_reply\n",
    "\n",
    "\n",
    "  messages.append({\"role\": \"assistant\", \"content\": gather_reply})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ed115ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "* Running on public URL: https://ad3daa0a2558922296.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ad3daa0a2558922296.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://ad3daa0a2558922296.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "  gr.Markdown(\"# Chat with LLAMA\")\n",
    "  with gr.Row():\n",
    "    with gr.Column():\n",
    "      user_input = gr.Textbox(label=\"Your Message Here\", placeholder=\"Type Something here....\")\n",
    "      output_box = gr.Markdown(label=\"AI Response\", min_height=50)\n",
    "      send_button = gr.Button(\"Send\")\n",
    "\n",
    "  send_button.click(fn=generate_gradio_opt, inputs=user_input, outputs=output_box)\n",
    "\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
